<p align='justify'>
I am a PhD Student at the
<b>Autonomous University of Madrid</b>
and study foundational topics in <b>machine learning</b> and
<b>function space inference</b>.
My research focuses on function space inference, more precisely on studying
implicit processes and methods of approximating the intractable distributions
they define.
</p><br>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td>
      <span class='cvdate'>2020&nbsp;-&nbsp;2022</span>
      <strong>MsC in Data Science</strong>, <em>Autonomous University</em>
      <br>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2015&nbsp;-&nbsp;2020</span>
      <strong>B.S. in Computer Science</strong>, <em>University of Granada</em>
      <br>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2015&nbsp;-&nbsp;2020</span>
      <strong>B.S. in Mathematics</strong>, <em>University of Granada</em>
      <br>
    </td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Previous Positions
<table class="table table-hover">
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2021&nbsp;-&nbsp;2021</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>University of Almería</em></p>
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Publications

<!-- [<a href="https://github.com/bamos/cv/blob/master/publications/all.bib">BibTeX</a>] -->
Representative publications that I am a primary author on are
<span style='background-color: #ffffd0'>highlighted.</span>
<br>
[<a href="https://scholar.google.com/citations?user=1Ly8qeoAAAAJ">Google Scholar</a>; 2+ citations, h-index: 1+]

<h2>2022</h2>
<table class="table table-hover">

<tr id="tr-pmlr-v151-ortega22a" style="background-color: #ffffd0">
<td align='right' style='padding-left:0;padding-right:0;'>
1.
</td>
<td>
<a href='https://proceedings.mlr.press/v151/ortega22a.html' target='_blank'><img src="images/publications/pmlr-v151-ortega22a.png" onerror="this.style.display='none'" class="publicationImg" /></a> 
<em><a href='https://proceedings.mlr.press/v151/ortega22a.html' target='_blank'>Diversity and Generalization in Neural Network Ensembles</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_pmlr-v151-ortega22a").toggle()'>abs</a>] [<a href='https://github.com/PGM-Lab/2022-AISTATS-diversity' target='_blank'>code</a>] <br>
Luis&nbsp;A.&nbsp;Ortega, Rafael&nbsp;Cabañas, and Andrés&nbsp;Masegosa<br>
NeurIPS 2022  <br>

<div id="abs_pmlr-v151-ortega22a" style="text-align: justify; display: none" markdown="1">
Ensembles are widely used in machine learning and, usually, provide state-of-the-art performance in many prediction tasks. From the very beginning, the diversity of an ensemble has been identified as a key factor for the superior performance of these models. But the exact role that diversity plays in ensemble models is poorly understood, specially in the context of neural networks. In this work, we combine and expand previously published results in a theoretically sound framework that describes the relationship between diversity and ensemble performance for a wide range of ensemble methods. More precisely, we provide sound answers to the following questions: how to measure diversity, how diversity relates to the generalization error of an ensemble, and how diversity is promoted by neural network ensemble algorithms. This analysis covers three widely used loss functions, namely, the squared loss, the cross-entropy loss, and the 0-1 loss; and two widely used model combination strategies, namely, model averaging and weighted majority vote. We empirically validate this theoretical analysis with neural network ensembles.
</div>

</td>
</tr>


<tr id="tr-santana2022correcting" >
<td align='right' style='padding-left:0;padding-right:0;'>
2.
</td>
<td>
<a href='https://arxiv.org/abs/2207.10673' target='_blank'><img src="images/publications/santana2022correcting.png" onerror="this.style.display='none'" class="publicationImg" /></a> 
<em><a href='https://arxiv.org/abs/2207.10673' target='_blank'>Correcting Model Bias with Sparse Implicit Processes</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_santana2022correcting").toggle()'>abs</a>]<br>
Sim&oacute;n&nbsp;Rodr&iacute;guez&nbsp;Santana, Luis&nbsp;A.&nbsp;Ortega, Daniel&nbsp;Hernández-Lobato, and Bryan&nbsp;Zald&iacute;var<br>
arXiv 2022  <br>

<div id="abs_santana2022correcting" style="text-align: justify; display: none" markdown="1">
Model selection in machine learning (ML) is a crucial part of the Bayesian learning procedure. Model choice may impose strong biases on the resulting predictions, which can hinder the performance of methods such as Bayesian neural networks and neural samplers. On the other hand, newly proposed approaches for Bayesian ML exploit features of approximate inference in function space with implicit stochastic processes (a generalization of Gaussian processes). The approach of Sparse Implicit Processes (SIP) is particularly successful in this regard, since it is fully trainable and achieves flexible predictions. Here, we expand on the original experiments to show that SIP is capable of correcting model bias when the data generating mechanism differs strongly from the one implied by the model. We use synthetic datasets to show that SIP is capable of providing predictive distributions that reflect the data better than the exact predictions of the initial, but wrongly assumed model.
</div>

</td>
</tr>

</table>


## <i class="fa fa-chevron-right"></i> Open Source Repositories
<table class="table table-hover">
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>1.</td>
  <td>
    <span class='cvdate'>2022</span>
    <a href="https://github.com/PGM-Lab/2022-AISTATS-diversity">PGM-Lab/2022-AISTATS-diversity</a> |
    <i class="fa fas fa-star"></i> 2 |
    <em>Diversity and Generalization on Neural Network Ensembles</em>
    <!--  -->
    <!--     PGM-Lab/2022-AISTATS-diversity  -->
    <!--  -->
  </td>
</tr>
</table>
