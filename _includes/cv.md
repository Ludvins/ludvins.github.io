<p align='justify'>
I am a Postdoctoral Researcher
<b>University of Aalborg</b>
and study foundational topics in <b>Probabilistic Machine Learning</b> and
<b>Variational Inference</b>.
My research focuses on Function-Space Variational Inference, Linearized Laplace Approximation, Deep Ensembles, and Chernoff-Based Generalization Bounds.
</p><br>


## <i class="fa fa-chevron-right"></i> Current Position

<table class="table table-hover">
  <tr>
    <td style='padding-right:0;'>
      <span class='cvdate'>2026&nbsp;-&nbsp;Present</span>
      <p markdown="1" style='margin: 0'><strong>Postdoctoral Researcher</strong>, <em>University of Aalborg</em>          , Denmark
</p>
    </td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Previous Positions
<table class="table table-hover">
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>11/2021&nbsp;-&nbsp;11/2025</span>
<p markdown="1" style='margin: 0'><strong>Research Personnel</strong>, <em>Autonomous University of Madrid</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(Research on Variational Function-space methods for Bayesian Deep Learning, and Generalization bounds for Machine Learning.)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>09/2023&nbsp;-&nbsp;12/2023</span>
<p markdown="1" style='margin: 0'><strong>Visitor Researcher</strong>, <em>University of Cambridge</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(Research on Uncertainty Estimation on Large Language Models with <a href="https://jmhl.org/" target="_blank">José Miguel Hernández Lobato</a>.)
</span></p>
  </td>
</tr>
<tr>
  <td style='padding-right:0;'>
<span class='cvdate'>2021&nbsp;-&nbsp;2021</span>
<p markdown="1" style='margin: 0'><strong>Research Assistant</strong>, <em>University of Almería</em><span markdown="1" style="color:grey;font-size:1.3rem;margin: 0">
(Research on the effect of diversity on Deep Neural Network ensembles with <a href="https://andresmasegosa.github.io/" target="_blank">Andrés R. Masegosa</a>.)
</span></p>
  </td>
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td>
      <span class='cvdate'>11/2021&nbsp;-&nbsp;11/2025</span>
      <strong>Ph.D. Student</strong>, <em>Autonomous University of Madrid</em>
      <br>
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> Thesis: *Uncertainty Estimation and Generalization Bounds for Modern Deep Learning*
        </p>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2024&nbsp;-&nbsp;202X</span>
      <strong>B.S. in Physics</strong>, <em>National Distance Education University</em>
      <br>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2020&nbsp;-&nbsp;2022</span>
      <strong>M.S. in Data Science</strong>, <em>Autonomous University of Madrid</em>
      <br>
        <p style='margin-top:-1em;margin-bottom:0em' markdown='1'>
        <br> Master Thesis: *Deep Variational Implicit Processes*
        </p>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2015&nbsp;-&nbsp;2020</span>
      <strong>B.S. in Computer Science</strong>, <em>University of Granada</em>
      <br>
    </td>
  </tr>
  <tr>
    <td>
      <span class='cvdate'>2015&nbsp;-&nbsp;2020</span>
      <strong>B.S. in Mathematics</strong>, <em>University of Granada</em>
      <br>
    </td>
  </tr>
</table>


## <i class="fa fa-chevron-right"></i> Publications

<!-- [<a href="https://github.com/bamos/cv/blob/master/publications/all.bib">BibTeX</a>] -->
<!-- Representative publications that I am a primary author on are -->
<!-- <span style='background-color: #ffffd0'>highlighted.</span> -->
<br>
<!-- [<a href="https://scholar.google.com/citations?user=1Ly8qeoAAAAJ">Google Scholar</a>; 14+ citations, h-index: 1+] -->

<h2>2026</h2>
<table class="table table-hover">

<tr id="tr-ortega2026scalable" >
<td align='right' style='padding-left:0;padding-right:0;'>
1.
</td>
<td>
<a href='' target='_blank'><img src="images/publications/ortega2026scalable.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='' target='_blank'>Scalable Linearized Laplace Approximation via Surrogate Neural Kernel</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ortega2026scalable").toggle()'>abs</a>] [<a href='https://github.com/Ludvins/BayesiPy' target='_blank'>code</a>] <br>
<strong>Luis&nbsp;A&nbsp;Ortega</strong>, <a href='https://www.iit.comillas.edu/personas/srsantana' target='_blank'>Simón&nbsp;Rodríguez-Santana</a>, and <a href='https://dhnzl.org' target='_blank'>Daniel&nbsp;Hernández-Lobato</a><br>
European Symposium on Artificial Neural Networks (ESANN). Spotlight talk. 2026  <br>

<div id="abs_ortega2026scalable" style="text-align: justify; display: none" markdown="1">
<br> 
We introduce a scalable method to approximate the kernel of the Linearized Laplace Approximation (LLA). For this, we use a surrogate deep neural network (DNN) that learns a compact feature representation whose inner product replicates the Neural Tangent Kernel (NTK). This avoids the need to compute large Jacobians. Training relies solely on efficient Jacobian-vector products, allowing to compute predictive uncertainty on large-scale pre-trained DNNs. Experimental results show similar or improved uncertainty estimation and calibration compared to existing LLA approximations. Notwithstanding, biasing the learned kernel significantly enhances out-of-distribution detection. This remarks the benefits of the proposed method for finding better kernels than the NTK in the context of LLA to compute prediction uncertainty given a pre-trained DNN.
</div>

</td>
</tr>


<tr id="tr-jimenez2026improving" >
<td align='right' style='padding-left:0;padding-right:0;'>
2.
</td>
<td>
<a href='' target='_blank'><img src="images/publications/jimenez2026improving.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='' target='_blank'>Improving the Linearized Laplace Approximation via Quadratic Approximations</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_jimenez2026improving").toggle()'>abs</a>] [<a href='' target='_blank'>code</a>] <br>
Pedro&nbsp;Jiménez, <strong>Luis&nbsp;A&nbsp;Ortega</strong>, Pablo&nbsp;Morales-Álvarez, and <a href='https://dhnzl.org' target='_blank'>Daniel&nbsp;Hernández-Lobato</a><br>
European Symposium on Artificial Neural Networks (ESANN) 2026  <br>

<div id="abs_jimenez2026improving" style="text-align: justify; display: none" markdown="1">
<br> 
Deep neural networks (DNNs) often produce overconfident out-of-distribution predictions, motivating Bayesian uncertainty quantification. The Linearized Laplace Approximation (LLA) achieves this by linearizing the DNN and applying Laplace inference to the resulting model. Importantly, the linear model is also used for prediction. We argue this linearization in the posterior may degrade fidelity to the true Laplace approximation. To alleviate this problem, without increasing significantly the computational cost, we propose the Quadratic Laplace Approximation (QLA). QLA approximates each second order factor in the approximate Laplace log-posterior using a rank-one factor obtained via efficient power iterations. QLA is expected to yield a posterior precision closer to that of the full Laplace without forming the full Hessian, which is typically intractable. For prediction, QLA also uses the linearized model. Empirically, QLA yields modest yet consistent uncertainty estimation improvements over LLA on five regression datasets.
</div>

</td>
</tr>


<tr id="tr-ortega2026slarge" >
<td align='right' style='padding-left:0;padding-right:0;'>
3.
</td>
<td>
<a href='' target='_blank'><img src="images/publications/ortega2026slarge.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='' target='_blank'>A Large Deviation Theory Analysis on the Implicit Bias of SGD</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ortega2026slarge").toggle()'>abs</a>] [<a href='https://github.com/Ludvins/sgd-implicit-bias-ldt' target='_blank'>code</a>] <br>
<strong>Luis&nbsp;A.&nbsp;Ortega</strong> and <a href='https://andresmasegosa.github.io/' target='_blank'>Andrés&nbsp;R.&nbsp;Masegosa</a><br>
Neurocomputing 2026  <br>

<div id="abs_ortega2026slarge" style="text-align: justify; display: none" markdown="1">
<br> 
Stochastic Gradient Descent (SGD) is the primary optimization method used in deep learning, yet the reasons behind its ability to select models that generalize effectively remain unclear. This paper develops a new perspective based on Large Deviation Theory (LDT). We show that the generalization error can be decomposed into three terms: the expected loss, a component that reflects the concentration of the empirical loss around its mean, and a component that captures the abnormality of deviations arising from stochastic sampling. This decomposition highlights a key difference between optimization methods: while full-batch Gradient Descent tends to exploit poorly concentrated and abnormal fluctuations—often leading to overfitting—mini-batch SGD naturally biases the search towards models with tighter concentration and fewer abnormal deviations. The analysis relies on standard assumptions such as i.i.d data and smooth loss functions. Experiments with deep convolutional networks support the theoretical findings, showing that smaller batch sizes and l2 regularization reinforce the preference for models with smaller generalization error. These results position LDT as a useful tool for understanding implicit regularization in SGD and suggest directions for extending this perspective to broader machine learning settings.
</div>

</td>
</tr>

</table>
<h2>2025</h2>
<table class="table table-hover">

<tr id="tr-masegosa2024pacchernoff" >
<td align='right' style='padding-left:0;padding-right:0;'>
4.
</td>
<td>
<a href='https://www.jair.org/index.php/jair/article/view/17036' target='_blank'><img src="images/publications/masegosa2024pacchernoff.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://www.jair.org/index.php/jair/article/view/17036' target='_blank'>PAC-Chernoff Bounds: Understanding Generalization in the Interpolation Regime</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_masegosa2024pacchernoff").toggle()'>abs</a>] [<a href='https://github.com/Ludvins/2024_PAC-Chernoff-Bound' target='_blank'>code</a>] <br>
<a href='https://andresmasegosa.github.io/' target='_blank'>Andrés&nbsp;R.&nbsp;Masegosa</a> and <strong>Luis&nbsp;A.&nbsp;Ortega</strong><br>
Journal of Artificial Intelligence Research (JAIR). Spotlight talk at European Conference on Artificial Intelligence (ECAI) 2025  <br>

<div id="abs_masegosa2024pacchernoff" style="text-align: justify; display: none" markdown="1">
<br> 
This paper introduces a distribution-dependent PAC-Chernoff bound that exhibits perfect tightness for interpolators, even within over-parameterized model classes. This bound, which relies on basic principles of Large Deviation Theory, defines a natural measure of the smoothness of a model, characterized by simple real-valued functions. Building upon this bound and the new concept of smoothness, we present an unified theoretical framework revealing why certain interpolators show an exceptional generalization, while others falter. We theoretically show how a wide spectrum of modern learning methodologies, encompassing techniques such as l2-norm, distance-from-initialization and input-gradient regularization, in combination with data augmentation, invariant architectures, and over-parameterization, collectively guide the optimizer toward smoother interpolators, which, according to our theoretical framework, are the ones exhibiting superior generalization performance. This study shows that distribution-dependent bounds serve as a powerful tool to understand the complex dynamics behind the generalization capabilities of over-parameterized interpolators.
</div>

</td>
</tr>

</table>
<h2>2024</h2>
<table class="table table-hover">

<tr id="tr-casado2024pacbayeschernoff" >
<td align='right' style='padding-left:0;padding-right:0;'>
5.
</td>
<td>
<a href='https://openreview.net/forum?id=CyzZeND3LB' target='_blank'><img src="images/publications/casado2024pacbayeschernoff.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://openreview.net/forum?id=CyzZeND3LB' target='_blank'>PAC-Bayes-Chernoff Bounds for Unbounded Losses</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_casado2024pacbayeschernoff").toggle()'>abs</a>]<br>
<a href='https://www.bcamath.org/es/node/11954' target='_blank'>Ioar&nbsp;Casado</a>, <strong>Luis&nbsp;A.&nbsp;Ortega</strong>, <a href='https://www.bcamath.org/es/node/10660' target='_blank'>Aritz&nbsp;Pérez</a>, and <a href='https://andresmasegosa.github.io/' target='_blank'>Andrés&nbsp;R.&nbsp;Masegosa</a><br>
Neural Information Processing Systems (NeurIPS) 2024  <br>

<div id="abs_casado2024pacbayeschernoff" style="text-align: justify; display: none" markdown="1">
<br> 
We introduce a new PAC-Bayes oracle bound for unbounded losses that extends Cramér-Chernoff bounds to the PAC-Bayesian setting. The proof technique relies on controlling the tails of certain random variables involving the Cramér transform of the loss. Our approach naturally leverages properties of Cramér-Chernoff bounds, such as exact optimization of the free parameter in many PAC-Bayes bounds. We highlight several applications of the main theorem. Firstly, we show that our bound recovers and generalizes previous results. Additionally, our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new model-dependent assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. Notably, many of these bounds can be minimized to obtain distributions beyond the Gibbs posterior and provide novel theoretical coverage to existing regularization techniques.
</div>

</td>
</tr>


<tr id="tr-ortega2024variational" >
<td align='right' style='padding-left:0;padding-right:0;'>
6.
</td>
<td>
<a href='https://proceedings.mlr.press/v235/ortega24a.html' target='_blank'><img src="images/publications/ortega2024variational.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://proceedings.mlr.press/v235/ortega24a.html' target='_blank'>Variational Linearized Laplace Approximation for Bayesian Deep Learning</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ortega2024variational").toggle()'>abs</a>] [<a href='https://github.com/Ludvins/Variational-LLA' target='_blank'>code</a>] <br>
<strong>Luis&nbsp;A.&nbsp;Ortega</strong>, <a href='https://www.iit.comillas.edu/personas/srsantana' target='_blank'>Simón&nbsp;Rodríguez-Santana</a>, and <a href='https://dhnzl.org' target='_blank'>Daniel&nbsp;Hernández-Lobato</a><br>
International Conference on Machine Learning (ICML) 2024  <br>

<div id="abs_ortega2024variational" style="text-align: justify; display: none" markdown="1">
<br> 
The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains, as the predictive mean, the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our proposed method against accelerated LLA (ELLA), which relies on the Nyström approximation, as well as other LLA variants employing the sample-then-optimize principle. Experimental results, both on regression and classification datasets, show that our method outperforms these already existing efficient variants of LLA, both in terms of the quality of the predictive distribution and in terms of total computational time.
</div>

</td>
</tr>


<tr id="tr-zhang2024cold" >
<td align='right' style='padding-left:0;padding-right:0;'>
7.
</td>
<td>
<a href='https://openreview.net/forum?id=GZORXGxHHT' target='_blank'><img src="images/publications/zhang2024cold.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://openreview.net/forum?id=GZORXGxHHT' target='_blank'>The Cold Posterior Effect Indicates Underfitting</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_zhang2024cold").toggle()'>abs</a>] [<a href='https://github.com/pyijiezhang/cpe-underfit' target='_blank'>code</a>] <br>
<a href='https://sites.google.com/view/yijiezhang/home' target='_blank'>Yijie&nbsp;Zhang</a>, <a href='https://sites.google.com/view/yishanwu/home' target='_blank'>Yi-Shan&nbsp;Wu</a>, <strong>Luis&nbsp;A.&nbsp;Ortega</strong>, and <a href='https://andresmasegosa.github.io/' target='_blank'>Andrés&nbsp;R.&nbsp;Masegosa</a><br>
Transactions for Machine Learning Research (TMLR) 2024  <br>

<div id="abs_zhang2024cold" style="text-align: justify; display: none" markdown="1">
<br> 
The cold posterior effect (CPE) (Wenzel et al., 2020) in Bayesian deep learning shows that, for posteriors with a temperature T<1, the resulting posterior predictive could have better performance than the Bayesian posterior (T=1). As the Bayesian posterior is known to be optimal under perfect model specification, many recent works have studied the presence of CPE as a model misspecification problem, arising from the prior and/or from the likelihood. In this work, we provide a more nuanced understanding of the CPE as we show that misspecification leads to CPE only when the resulting Bayesian posterior underfits. In fact, we theoretically show that if there is no underfitting, there is no CPE. Furthermore, we show that these tempered posteriors with (T<1) are indeed proper Bayesian posteriors with a different combination of likelihood and prior parameterized by T. This observation validates the adjustment of the temperature hyperparameter T as a straightforward approach to mitigate underfitting in the Bayesian posterior. In essence, we show that by fine-tuning the temperature T we implicitly utilize alternative Bayesian posteriors, albeit with less misspecified likelihood and prior distributions.
</div>

</td>
</tr>

</table>
<h2>2023</h2>
<table class="table table-hover">

<tr id="tr-ortega2023deep" >
<td align='right' style='padding-left:0;padding-right:0;'>
8.
</td>
<td>
<a href='https://openreview.net/forum?id=8aeSJNbmbQq' target='_blank'><img src="images/publications/ortega2023deep.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://openreview.net/forum?id=8aeSJNbmbQq' target='_blank'>Deep Variational Implicit Processes</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_ortega2023deep").toggle()'>abs</a>] [<a href='https://github.com/Ludvins/2023-ICLR-DVIP' target='_blank'>code</a>] <br>
<strong>Luis&nbsp;A.&nbsp;Ortega</strong>, <a href='https://www.iit.comillas.edu/personas/srsantana' target='_blank'>Simón&nbsp;Rodríguez-Santana</a>, and <a href='https://dhnzl.org' target='_blank'>Daniel&nbsp;Hernández-Lobato</a><br>
International Conference on Learning Representations (ICLR) 2023  <br>

<div id="abs_ortega2023deep" style="text-align: justify; display: none" markdown="1">
<br> 
Implicit processes (IPs) are a generalization of Gaussian processes (GPs). IPs may lack a closed-form expression but are easy to sample from. Examples include, among others, Bayesian neural networks or neural samplers. IPs can be used as priors over functions, resulting in flexible models with well-calibrated prediction uncertainty estimates. Methods based on IPs usually carry out function-space approximate inference, which overcomes some of the difficulties of parameter-space approximate inference. Nevertheless, the approximations employed often limit the expressiveness of the final model, resulting, e.g., in a Gaussian predictive distribution, which can be restrictive. We propose here a multi-layer generalization of IPs called the Deep Variational Implicit process (DVIP). This generalization is similar to that of deep GPs over GPs, but it is more flexible due to the use of IPs as the prior distribution over the latent functions. We describe a scalable variational inference algorithm for training DVIP and show that it outperforms previous IP-based methods and also deep GPs. We support these claims via extensive regression and classification experiments. We also evaluate DVIP on large datasets with up to several million data instances to illustrate its good scalability and performance.
</div>

</td>
</tr>

</table>
<h2>2022</h2>
<table class="table table-hover">

<tr id="tr-pmlr-v151-ortega22a" >
<td align='right' style='padding-left:0;padding-right:0;'>
9.
</td>
<td>
<a href='https://proceedings.mlr.press/v151/ortega22a.html' target='_blank'><img src="images/publications/pmlr-v151-ortega22a.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://proceedings.mlr.press/v151/ortega22a.html' target='_blank'>Diversity and Generalization in Neural Network Ensembles</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_pmlr-v151-ortega22a").toggle()'>abs</a>] [<a href='https://github.com/PGM-Lab/2022-AISTATS-diversity' target='_blank'>code</a>] <br>
<strong>Luis&nbsp;A.&nbsp;Ortega</strong>, <a href='https://www.linkedin.com/in/rcabanasdepaz' target='_blank'>Rafael&nbsp;Cabañas</a>, and <a href='https://andresmasegosa.github.io/' target='_blank'>Andrés&nbsp;R.&nbsp;Masegosa</a><br>
Artificial Intelligence and Statistics (AISTATS) 2022  <br>

<div id="abs_pmlr-v151-ortega22a" style="text-align: justify; display: none" markdown="1">
<br> 
Ensembles are widely used in machine learning and, usually, provide state-of-the-art performance in many prediction tasks. From the very beginning, the diversity of an ensemble has been identified as a key factor for the superior performance of these models. But the exact role that diversity plays in ensemble models is poorly understood, specially in the context of neural networks. In this work, we combine and expand previously published results in a theoretically sound framework that describes the relationship between diversity and ensemble performance for a wide range of ensemble methods. More precisely, we provide sound answers to the following questions: how to measure diversity, how diversity relates to the generalization error of an ensemble, and how diversity is promoted by neural network ensemble algorithms. This analysis covers three widely used loss functions, namely, the squared loss, the cross-entropy loss, and the 0-1 loss; and two widely used model combination strategies, namely, model averaging and weighted majority vote. We empirically validate this theoretical analysis with neural network ensembles.
</div>

</td>
</tr>


<tr id="tr-santana2022correcting" >
<td align='right' style='padding-left:0;padding-right:0;'>
10.
</td>
<td>
<a href='https://arxiv.org/abs/2207.10673' target='_blank'><img src="images/publications/santana2022correcting.png" onerror="this.style.display='none'" class="publicationImg"/></a> 
<em><a href='https://arxiv.org/abs/2207.10673' target='_blank'>Correcting Model Bias with Sparse Implicit Processes</a> </em> 
[<a href='javascript:;'
    onclick='$("#abs_santana2022correcting").toggle()'>abs</a>] [<a href='https://github.com/simonrsantana/sparse-implicit-processes' target='_blank'>code</a>] <br>
<a href='https://www.iit.comillas.edu/personas/srsantana' target='_blank'>Simón&nbsp;Rodríguez-Santana</a>, <strong>Luis&nbsp;A.&nbsp;Ortega</strong>, <a href='https://dhnzl.org' target='_blank'>Daniel&nbsp;Hernández-Lobato</a>, and <a href='https://www.linkedin.com/in/bryan-zaldivar/' target='_blank'>Bryan&nbsp;Zaldívar</a><br>
ICML Workshop "Beyond Bayes: Paths Towards Universal Reasoning Systems" 2022  <br>

<div id="abs_santana2022correcting" style="text-align: justify; display: none" markdown="1">
<br> 
Model selection in machine learning (ML) is a crucial part of the Bayesian learning procedure. Model choice may impose strong biases on the resulting predictions, which can hinder the performance of methods such as Bayesian neural networks and neural samplers. On the other hand, newly proposed approaches for Bayesian ML exploit features of approximate inference in function space with implicit stochastic processes (a generalization of Gaussian processes). The approach of Sparse Implicit Processes (SIP) is particularly successful in this regard, since it is fully trainable and achieves flexible predictions. Here, we expand on the original experiments to show that SIP is capable of correcting model bias when the data generating mechanism differs strongly from the one implied by the model. We use synthetic datasets to show that SIP is capable of providing predictive distributions that reflect the data better than the exact predictions of the initial, but wrongly assumed model.
</div>

</td>
</tr>

</table>


## <i class="fa fa-chevron-right"></i> Ongoing Research
<table class="table table-hover">
<tr>
  <td>
  <!-- <div style='float: right'></div> -->
  <div>
    Fixed-Mean Gaussian Processes for ad-hoc Bayesian Deep Learning (under review)
        [<a href="https://arxiv.org/abs/2412.04177">pre-print</a>]
    <br><p style="color:grey;font-size:1.4rem">Converting models to Bayesian by creating a Gaussian Process with fixed predictive mean to that model.</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'></td> -->
</tr>
<tr>
  <td>
  <!-- <div style='float: right'></div> -->
  <div>
    Regularization as Estimation, A PAC-Bayes-Chernoff Approach
    <br><p style="color:grey;font-size:1.4rem">A prescriptive framework, grounded in PAC-Bayes-Chernoff bounds, that reframes regularization as a statistical estimation problem.</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'></td> -->
</tr>
<tr>
  <td>
  <!-- <div style='float: right'></div> -->
  <div>
    Revisiting the Marginal Likelihood through a PAC-Bayesian lens
    <br><p style="color:grey;font-size:1.4rem">While marginal likelihood remains a critical component, generalization in Bayesian models depends on additional factors beyond marginal likelihood alone.</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'></td> -->
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Honors & Awards
<table class="table table-hover">
<tr>
  <td>
  <div style='float: right'>2023</div>
  <div>
    Granted Santander-UAM Scholarship. Uncertainty Estimation in LLM at Cambridge University.
    <br><p style="color:grey;font-size:1.4rem">Computational and Biological Learning Lab, University of Cambridge</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2023</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2021</div>
  <div>
    Granted FPI-UAM Scholarship. Competitive Predoctoral Contract for Training Research Personnel
    <br><p style="color:grey;font-size:1.4rem">Department of Computer Science, Autonomous University of Madrid</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2021</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2020</div>
  <div>
    Research Collaboration Scholarship
    <br><p style="color:grey;font-size:1.4rem">Department of Computer Science, Autonomous University of Madrid</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2020</td> -->
</tr>
<tr>
  <td>
  <div style='float: right'>2020</div>
  <div>
    Granted Highest Mark on Bachelor's Thesis, 10/10. Statistical Models with Variational Methods
    <br><p style="color:grey;font-size:1.4rem">Department of Computer Science and Faculty of Science, University of Granada</p>
  </div>
  </td>
  <!-- <td class='col-md-2' style='text-align:right;'>2020</td> -->
</tr>
</table>


## <i class="fa fa-chevron-right"></i> Open Source Contributions
<table class="table table-hover">
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>1.</td>
  <td>
    <span class='cvdate'>2025</span>
    <a href="https://github.com/Ludvins/Bayesipy">Ludvins/Bayesipy</a> |
    <i class="fa fas fa-star"></i> 19 |
    <em>Full post-hoc suite that includes Variational (VaLLA) and Nyström (ELLA) LLA variants, FMGPs, SNGPs abd MFVI. Along with loaderrs for benchmarking on pre-defined data and models.</em>
    <!--  -->
    <!--     Ludvins/Bayesipy  -->
    <!--  -->
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>2.</td>
  <td>
    <span class='cvdate'>2024</span>
    <a href="https://github.com/AlexImmer/Laplace">AlexImmer/Laplace</a> |
    <i class="fa fas fa-star"></i> 441 |
    <em>Implemented Functional (GP) Laplace.</em>
    <!--  -->
    <!--     AlexImmer/Laplace  -->
    <!--  -->
  </td>
</tr>
<tr>
  <td align='right' style='padding-right:0;padding-left:0;'>3.</td>
  <td>
    <span class='cvdate'>2017</span>
    <a href="https://github.com/libreim/apuntesDGIIM">libreim/apuntesDGIIM</a> |
    <i class="fa fas fa-star"></i> 79 |
    <em>Divulgation group destinated to the double degree in computer science and mathematics, Granada.</em>
    <!--  -->
    <!--     libreim/apuntesDGIIM  -->
    <!--  -->
  </td>
</tr>
</table>
